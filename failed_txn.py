# -*- coding: utf-8 -*-
"""Failed Txn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xbKeR_8gHhClZXdIZFIlhZqGtj6mG885
"""

import pandas as pd

data=pd.read_csv('/content/synthetic_fraud_dataset.csv')
data.head()

data = data.drop(['Transaction_ID', 'User_ID', 'Timestamp', 'Location'], axis=1)
display(data.head())

display(data.duplicated().sum())

display(data.isnull().sum())

from sklearn.preprocessing import LabelEncoder

categorical_cols = ['Transaction_Type', 'Device_Type', 'Merchant_Category', 'Card_Type', 'Authentication_Method']
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

display(data.head())

# Define features (X) and target variable (y)
X = data.drop('Fraud_Label', axis=1)
y = data['Fraud_Label']

display("Shape of X:")
display(X.shape)
display("Shape of y:")
display(y.shape)

data.info()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

display(X_train.shape)
display(X_test.shape)
display(y_train.shape)
display(y_test.shape)

import statsmodels.api as sm

# Add a constant to the independent variables
X_train_sm = sm.add_constant(X_train)

# Fit the logistic regression model
model = sm.Logit(y_train, X_train_sm)
result = model.fit()

# Display the summary of the model
display(result.summary())

from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score
import numpy as np

# Add a constant to the test data
X_test_sm = sm.add_constant(X_test, has_constant='add')

# Get predicted probabilities
y_pred_prob = result.predict(X_test_sm)

# Convert probabilities to binary predictions using a threshold (e.g., 0.5)
y_pred = (y_pred_prob > 0.5).astype(int)

# Calculate evaluation metrics
conf_matrix = confusion_matrix(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_prob)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Display the metrics
display("Confusion Matrix:")
display(conf_matrix)
display("AUC:")
display(auc)
display("Precision:")
display(precision)
display("Recall:")
display(recall)
display("F1 Score:")
display(f1)

display(data['Fraud_Label'].value_counts())

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

display("Shape of X_train before SMOTE:")
display(X_train.shape)
display("Shape of X_train after SMOTE:")
display(X_train_resampled.shape)
display("Value counts of y_train before SMOTE:")
display(y_train.value_counts())
display("Value counts of y_train after SMOTE:")
display(y_train_resampled.value_counts())

import statsmodels.api as sm

# Add a constant to the resampled independent variables
X_train_resampled_sm = sm.add_constant(X_train_resampled)

# Fit the logistic regression model on the resampled data
model_resampled = sm.Logit(y_train_resampled, X_train_resampled_sm)
result_resampled = model_resampled.fit()

# Display the summary of the model
display(result_resampled.summary())

"""Let's implement a basic forward stepwise selection for logistic regression using `statsmodels`. This process will start with no features and iteratively add the feature that most improves the model, based on its p-value, until no remaining feature has a p-value below a certain threshold (commonly 0.05)."""

import statsmodels.api as sm

def forward_stepwise_selection(X, y, initial_list=[], threshold_in=0.05):
    """
    Performs forward stepwise logistic regression.

    Parameters:
    -----------
    X : pandas.DataFrame
        The independent variables.
    y : pandas.Series or numpy.ndarray
        The dependent variable.
    initial_list : list, optional
        Initial list of features to start with. Defaults to an empty list.
    threshold_in : float, optional
        The p-value threshold for adding a feature. Defaults to 0.05.

    Returns:
    --------
    list
        The selected features.
    """
    included = list(initial_list)
    while True:
        changed = False
        # forward step
        excluded = list(set(X.columns) - set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.Logit(y, sm.add_constant(X[included + [new_column]]))
            result = model.fit(disp=0) # disp=0 to suppress convergence output
            new_pval[new_column] = result.pvalues[new_column]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True
            print(f'Add {best_feature} with p-value {best_pval:.4f}')

        if not changed:
            break
    return included

# Perform forward stepwise selection on the resampled training data
selected_features = forward_stepwise_selection(X_train_resampled, y_train_resampled)

display("Selected features through stepwise regression:")
display(selected_features)

"""Now that we have the selected features, we can train the logistic regression model using only these features and then evaluate its performance."""

# Train the logistic regression model with selected features
X_train_selected = X_train_resampled[selected_features]
X_train_selected_sm = sm.add_constant(X_train_selected)

model_selected = sm.Logit(y_train_resampled, X_train_selected_sm)
result_selected = model_selected.fit()

# Display the summary of the model with selected features
display(result_selected.summary())

from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score
import numpy as np
import statsmodels.api as sm

# Select the features in the test set that were selected during stepwise regression
X_test_selected = X_test[selected_features]

# Add a constant to the selected test data
X_test_selected_sm = sm.add_constant(X_test_selected, has_constant='add')

# Get predicted probabilities
y_pred_prob_selected = result_selected.predict(X_test_selected_sm)

# Convert probabilities to binary predictions using a threshold (e.g., 0.5)
y_pred_selected = (y_pred_prob_selected > 0.5).astype(int)

# Calculate evaluation metrics
conf_matrix_selected = confusion_matrix(y_test, y_pred_selected)
auc_selected = roc_auc_score(y_test, y_pred_prob_selected)
precision_selected = precision_score(y_test, y_pred_selected)
recall_selected = recall_score(y_test, y_pred_selected)
f1_selected = f1_score(y_test, y_pred_selected)

# Display the metrics
display("Confusion Matrix (Selected Features):")
display(conf_matrix_selected)
display("AUC (Selected Features):")
display(auc_selected)
display("Precision (Selected Features):")
display(precision_selected)
display("Recall (Selected Features):")
display(recall_selected)
display("F1 Score (Selected Features):")
display(f1_selected)

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

# Create a scikit-learn Logistic Regression model
model_sklearn = LogisticRegression(random_state=42, solver='liblinear') # Using liblinear solver for simplicity with small datasets

# Perform cross-validation
# Using the resampled training data and the scikit-learn logistic regression model
# We can specify scoring metrics like 'roc_auc', 'precision', 'recall', 'f1'
# Let's use 'roc_auc' as an example, given the imbalanced nature
cv_scores = cross_val_score(model_sklearn, X_train_resampled, y_train_resampled, cv=5, scoring='roc_auc') # Using 5 folds

display("Cross-validation AUC scores:")
display(cv_scores)
display(f"Mean cross-validation AUC: {cv_scores.mean():.4f}")
display(f"Standard deviation of cross-validation AUC: {cv_scores.std():.4f}")

from sklearn.linear_model import LogisticRegression

# Create a scikit-learn Logistic Regression model
model_sklearn = LogisticRegression(random_state=42, solver='liblinear') # Using liblinear solver for simplicity with small datasets

# Train the scikit-learn model on the resampled training data
model_sklearn.fit(X_train_resampled, y_train_resampled)

display("Scikit-learn Logistic Regression model trained.")

import shap

# Assuming model_sklearn is your trained scikit-learn LogisticRegression model
# And X_test is your test data (without the target variable)

# Create a SHAP LinearExplainer for scikit-learn LogisticRegression
# The explainer needs the trained model and the data
explainer = shap.LinearExplainer(model_sklearn, X_train_resampled) # Use training data for background distribution

# Calculate SHAP values for the test set
# The explainer expects data in the same format as the training data
shap_values = explainer.shap_values(X_test)

# Visualize the summary plot
# Use the original feature names from X_test
shap.summary_plot(shap_values, X_test, feature_names=X_test.columns.tolist())

from sklearn.tree import DecisionTreeClassifier

# Train a Decision Tree Classifier model on the resampled training data with hyperparameters
dt_model = DecisionTreeClassifier(max_depth=6, min_samples_leaf=100, random_state=42)
dt_model.fit(X_train_resampled, y_train_resampled)

display("Decision Tree Classifier model trained with hyperparameters.")

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Visualize the Decision Tree Classifier
plt.figure(figsize=(20, 10)) # Adjust figure size as needed
plot_tree(dt_model,
          feature_names=X_train_resampled.columns.tolist(),
          class_names=['Not Fraud', 'Fraud'],
          filled=True,
          rounded=True,
          fontsize=8)
plt.title("Decision Tree Classifier Visualization")
plt.show()

import joblib
joblib.dump(model, 'FailedTxn_DT.pkl')
print("Model saved as FailedTxn_DT.pkl")

